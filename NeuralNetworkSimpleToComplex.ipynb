{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworkSimpleToComplex.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielhampikian/GIMM-400/blob/master/NeuralNetworkSimpleToComplex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbKpfYxHSs3Q",
        "colab_type": "text"
      },
      "source": [
        "NumPy is the main package for scientific computations in python and has been a major backbone of Python applications in various computational, engineering, scientific, statistical, image processing, etc fields. NumPy was built from 2 earlier libraries: Numeric and Numarray.\n",
        "Most deep learning algorithms make use of several numpy operations and functions. This is because compared with pure python syntax, NumPy computations are faster. NumPy for instance makes use of vectorization that enables the elimination of unnecessary loops in a code structure, hence reducing latency in execution of code. The following is an example of vectorization for a 1-d array NumPy dot operation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0hB0jFaS0B5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "f4d461d8-6731-4295-9a85-1babd388ff45"
      },
      "source": [
        "#with vectorization\n",
        "import numpy as np #always remember to import numpy\n",
        "import time\n",
        "t0 = time.process_time()\n",
        "#array1\n",
        "tmatrix1 = [1,2,3,4,5]\n",
        "#array2\n",
        "tmatrix2 = [6,7,8,9,10]\n",
        "#dot matrix\n",
        "dt = np.dot(tmatrix1,tmatrix2)\n",
        "t1 = time.process_time()\n",
        "print (\"dot operation = \" + str(dt) + \"\\n Computation time = \" + str(1000*(t1 - t0)) + \"ms\")\n",
        "\n",
        "#without vectorization\n",
        "t0 = time.process_time()\n",
        "dot = 0\n",
        "for i in range(len(tmatrix1)):\n",
        "  %time dot+= tmatrix1[i]*tmatrix2[i]\n",
        "t1 = time.process_time()\n",
        "print (\"dot operation = \" + str(dot) + \"\\n Computation time = \" + str(1000*(t1 - t0)) + \"ms\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dot operation = 130\n",
            " Computation time = 0.5991880000000283ms\n",
            "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
            "Wall time: 7.87 µs\n",
            "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
            "Wall time: 7.39 µs\n",
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 7.63 µs\n",
            "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
            "Wall time: 6.44 µs\n",
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 6.68 µs\n",
            "dot operation = 130\n",
            " Computation time = 6.979469000000016ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwosbnhFS6JL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "9d94294c-5971-4abc-beae-98549e00d2ef"
      },
      "source": [
        "import numpy as np\n",
        "#create a 1-d array of 1,2,3\n",
        "mvector = np.array([1,2,3])\n",
        "print(mvector)\n",
        "#create a 2x3 matrix/2-d array\n",
        "mmatrix= np.array([[1,2,3],[4,5,6]])\n",
        "print(mmatrix)\n",
        "#print array type\n",
        "print(type(mmatrix))\n",
        "#print array shape\n",
        "print(mmatrix.shape)\n",
        "#print array size\n",
        "print(mmatrix.size)\n",
        "#print smallest number in a matrix\n",
        "print(mmatrix.min())\n",
        "#print biggest number\n",
        "print(mmatrix.max())\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3]\n",
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "<class 'numpy.ndarray'>\n",
            "(2, 3)\n",
            "6\n",
            "1\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0VeDmQ4S6hR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8_n5sU5rtdt",
        "colab_type": "code",
        "outputId": "955e5c79-41eb-4fab-ef9e-f56e5dcee062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "#Adding, subtracting and multiplying\n",
        "#array1\n",
        "mmatrix1 = np.array([[1,2,3],[4,5,6]])\n",
        "#array2\n",
        "mmatrix2 = np.array([[6,7,8],[8,9,10]])\n",
        "#add matrices\n",
        "addmatrix = np.add(mmatrix1,mmatrix2)\n",
        "print(\"addmatrix \\n\", addmatrix)\n",
        "#subtract matrices\n",
        "submatrix = np.subtract(mmatrix1,mmatrix2)\n",
        "print(\"submatrix \\n\", submatrix)\n",
        "#multiply matrices\n",
        "mulmatrix = np.multiply(mmatrix1,mmatrix2)\n",
        "print(\"mulmatrix \\n\", mulmatrix)\n",
        "#Calculating dot products\n",
        "#array1\n",
        "dmatrix1 = np.array([[1,2,3],[4,5,6]])\n",
        "#array2\n",
        "dmatrix2 = np.array([[6,7],[8,9],[9,10]])\n",
        "print(\"array1 \\n\", dmatrix1)\n",
        "print(\"array2 \\n\", dmatrix2)\n",
        "#dot matrix\n",
        "dotmatrix = np.dot(dmatrix1,dmatrix2)\n",
        "print(\"dotmatrix \\n\", dotmatrix)\n",
        "#Finding the exponential of a matrix\n",
        "mexp = np.exp(mmatrix)\n",
        "print(mexp)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "addmatrix \n",
            " [[ 7  9 11]\n",
            " [12 14 16]]\n",
            "submatrix \n",
            " [[-5 -5 -5]\n",
            " [-4 -4 -4]]\n",
            "mulmatrix \n",
            " [[ 6 14 24]\n",
            " [32 45 60]]\n",
            "array1 \n",
            " [[1 2 3]\n",
            " [4 5 6]]\n",
            "array2 \n",
            " [[ 6  7]\n",
            " [ 8  9]\n",
            " [ 9 10]]\n",
            "dotmatrix \n",
            " [[ 49  55]\n",
            " [118 133]]\n",
            "[[  2.71828183   7.3890561   20.08553692]\n",
            " [ 54.59815003 148.4131591  403.42879349]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBsaA_WHSIqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "1584c26d-61cb-4a2a-9744-5111d2ec5db1"
      },
      "source": [
        "\n",
        "# Simple Feedforward Artificial Neural Network: no external libraries like tensorflow, pandas, or keras:\n",
        "# No backpropogation or optimization:\n",
        "\n",
        "epoch = 500 #number of times you will run the neural net through the entire training set\n",
        "lr = 1 #learning rate \n",
        "bias = 1 #value of bias \n",
        "weights = [random.random(),random.random(),random.random()] #weights generated in a list (3 weights in total for 2 neurons and the bias)\n",
        "print(\"Weights before training: \" + str(weights))\n",
        "\n",
        "def NeuralNet(weights, inputNeuron1, inputNeuron2, expectedOutput):\n",
        "  outputActual = inputNeuron1*weights[0]+inputNeuron2*weights[1]+bias*weights[2]\n",
        "  outputActual = ApplySigmoidActivation(outputActual)\n",
        "  error = expectedOutput - outputActual #simple error measurement\n",
        "  #inference: here we train the hidden layer which is just an array of weights.\n",
        "  weights[0] += error * inputNeuron1 * lr\n",
        "  weights[1] += error * inputNeuron2 * lr\n",
        "  weights[2] += error * bias * lr\n",
        "  \n",
        "def ApplySigmoidActivation(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "  \n",
        "def ApplySigmoidDerivative(x):\n",
        "  return  x*(1-x)\n",
        "\n",
        "def ApplyReluActivation(x):\n",
        "   return np.maximum(0,x)\n",
        "  \n",
        "def ApplyReluDerivative(x):\n",
        "  if x<=0:\n",
        "    x = 0\n",
        "  else:\n",
        "    x = 1\n",
        "  return x\n",
        "\n",
        "def softmax(x):\n",
        "  expo = np.exp(x)\n",
        "  expo_sum = np.sum(np.exp(x))\n",
        "  return expo/expo_sum\n",
        "\n",
        "print (softmax(mmatrix))\n",
        "\n",
        "\n",
        "for i in range(epoch):\n",
        "  NeuralNet(weights, 1,1,1) #True or true input should return true\n",
        "  NeuralNet(weights, 1,0,1) #True or false input should return true\n",
        "  NeuralNet(weights, 0,1,1) #False or true input should return true\n",
        "  NeuralNet(weights, 0,0,0) #False or false input should return false\n",
        "  if(i % 200== 0):\n",
        "    print(\"weights currently at: \" + str(weights))\n",
        "  \n",
        "print(\"Weights after training: \" + str(weights))\n",
        "while True: \n",
        "  x = int(input())\n",
        "  y = int(input())\n",
        "  output = x*weights[0] + y*weights[1] + bias*weights[2]\n",
        "  print(output)\n",
        "  if output > 0 : #softmax activation function for output because we have binary classification\n",
        "    output = 1\n",
        "  else :\n",
        "    output = 0\n",
        "  print(x, \"or\", y, \"is : \", output)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights before training: [0.6395260357636279, 0.7952832665343739, 0.1847190774741274]\n",
            "[[0.00426978 0.01160646 0.03154963]\n",
            " [0.08576079 0.23312201 0.63369132]]\n",
            "weights currently at: [1.0444121348448816, 1.1356165855960612, 0.08230210074235478]\n",
            "weights currently at: [8.30671219146593, 8.28807536868092, -3.695777540762057]\n",
            "weights currently at: [9.68204753525835, 9.67239026392353, -4.383165309024845]\n",
            "Weights after training: [10.122788219066265, 10.114992519428133, -4.603478599528888]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2b5b48b2d541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Weights after training: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEEZRWMWkeGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e59f5ef4-8387-4787-837c-7eb361a153aa"
      },
      "source": [
        "# Slightly more complex single layer Neural Network: still no external libraries like tensorflow, pandas, or keras:\n",
        "import numpy as np #import numpy the linear algebra library\n",
        "\n",
        "# sigmoid function - introduces non-linearity into the neural net (not 1-1 input output)\n",
        "#sigmoid maps to a value between 0 and 1 to convert numbers to probabilities\n",
        "def nonlin(x,deriv=False):\n",
        "    if(deriv==True): # when true, this maps the derivative of the sigmoid function - so we can get the slope at a given point\n",
        "        return x*(1-x)\n",
        "    return 1/(1+np.exp(-x))\n",
        "    \n",
        "# input dataset\n",
        "X = np.array([  [0,0,1],\n",
        "                [0,1,1],\n",
        "                [1,0,1],\n",
        "                [1,1,1]])\n",
        "    \n",
        "# output dataset            \n",
        "y = np.array([[0,0,1,1]]).T\n",
        "\n",
        "# seed random numbers to make calculation\n",
        "# deterministic (just a good practice)\n",
        "np.random.seed(1)\n",
        "epochs = 50000\n",
        "weights = np.array([[0,0,0,0]]).T\n",
        "# initialize weights randomly with mean 0\n",
        "#we only have an input and output layer, so we only need on matrix of wieghts to connect them with dimensions (3,1) 3 inputs to 1 output\n",
        "#if we wanted to change the number of inputs we would also change this (3,1) to reflect inputs, but we generally want 1 output since we're doing a regresion neural net\n",
        "#syn0 = 2*np.random.random((3,1)) - 1\n",
        "\n",
        "def neuralNet(l0, epochs, predict, userTest=[0,0,0], testWeights=[0,0,0,0]):\n",
        "  if(not predict):\n",
        "    for iter in range(epochs):\n",
        "\n",
        "      # forward propagation - the prediction step, we let the network try to predict it's output and measure the error\n",
        "      syn0 = testWeights\n",
        "      #this is the 'weighted sum step where we use the dot or cross product of two matrices to output a matrix that has the multiplied and summed total of the weights and values for that row and column combination, and then we convert that back into a number since we get a probability with the sigmoid function'\n",
        "      #the dimensions going on in the next step are: (4X3)dot(3X1) = (4X1) so everything in the matrix gets multplied and hte result is a matix with the number of rows in first matrix and number of columns in second\n",
        "      l1 = nonlin(np.dot(l0,syn0)) #multiplies 10 input by syn0 weights, then passes it through our sigmoid function to convert the numbers to probabilities \n",
        "      \n",
        "\n",
        "      # compare our guess - l1 - to the actual answer - y -\n",
        "      l1_error = y - l1\n",
        "      if(iter % 100 == 0):\n",
        "        print('error before slope of sigmoid of values in l1')\n",
        "        print(l1_error)\n",
        "        print(\"l1 itself (dot product of l0 and weight (weighted sum))\")\n",
        "        print(l1)\n",
        "    \n",
        "\n",
        "      # multiply how much we missed by the \n",
        "      # slope of the sigmoid at the values in l1 - basically we multiply elementwise a 4,1 matrix with another 4,1 matrix of its sigmoid derivatives thereby reducing the error of high confidence predictions.\n",
        "      # if the network has a very confident guess (slope is very shallow or close to 0), we leave it alone with the multiplication here but if it's closer to .5 then we heavily update the guess with this multiplication\n",
        "      l1_delta = l1_error * nonlin(l1,True)\n",
        "      if(iter%100 == 0):\n",
        "        print('error after slope of sigmoid of values in l1 is multiplied times l1 error')\n",
        "        print(l1_delta)\n",
        "        print(\"Mean Error:\" + str(np.mean(np.abs(l1_delta)))) # we'll just measure the error for accuracy here taking the mean of the absolute value of the errors in the array\n",
        "\n",
        "    \n",
        "\n",
        "     # update weights (again cross multiplication , added to all weights between the two matrix)\n",
        "      syn0 += np.dot(l0.T,l1_delta)\n",
        "    return syn0,l1\n",
        "    \n",
        "  if predict:           \n",
        "          \n",
        "    syn0 = testWeights\n",
        "    total = 0    #the dimensions goig on in the next step are: (4X3)dot(3X1) = (4X1) so everything in the matrix gets multplied and hte result is a matix with the number of rows in first matrix and number of columns in second\n",
        "    for i in range(len(userTest)):\n",
        "      #multiplies 10 input by syn0 weights, then passes it through our sigmoid function to convert the numbers to probabilities \n",
        "    #print(\"hey\" + str(l1))\n",
        "      total += userTest[i] * testWeights[i]\n",
        "        # compare our guess - l1 - to the actual answer - y -\n",
        "        # l1_error = y - l1\n",
        "        #l1_delta = l1_error * nonlin(l1,True)\n",
        "        \n",
        "    \n",
        "\n",
        "      # update weights (again cross multiplication , added to all weights between the two matrix)\n",
        "        #syn0 += np.dot(l0.T,l1_delta)\n",
        "     # return the prediction:\n",
        "    return np.around(nonlin(total),decimals=0)\n",
        "      \n",
        "w,l1 = neuralNet(X,1000, False, testWeights = 2*np.random.random((3,1)) - 1)\n",
        "print(\"Output After Training:\")\n",
        "print(str(l1))\n",
        "print(\"Weights: \" + str(w))\n",
        "#user test:\n",
        "while True:\n",
        "  userInput = input(\"Enter int array: \")\n",
        "  userArray = list(map(int, userInput.split()))\n",
        "  print(userArray)\n",
        "  print(str(neuralNet(X,1,True, userTest=userArray, testWeights=w)))\n",
        "#Note that adding a 0 0 0 learning set and a 0 1 0 doesn't work yet, we need to introduce two hyperparameters, layers, numbers of neurons, and an alpha or (learning rate)\n",
        "#Try 1 1 0 which is not in the data set, but if assume the rule it's trying to infer is perfect correspondence to the first column should return 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error before slope of sigmoid of values in l1\n",
            "[[-0.2689864 ]\n",
            " [-0.36375058]\n",
            " [ 0.76237183]\n",
            " [ 0.6737243 ]]\n",
            "l1 itself (dot product of l0 and weight (weighted sum))\n",
            "[[0.2689864 ]\n",
            " [0.36375058]\n",
            " [0.23762817]\n",
            " [0.3262757 ]]\n",
            "error after slope of sigmoid of values in l1 is multiplied times l1 error\n",
            "[[-0.05289153]\n",
            " [-0.08418501]\n",
            " [ 0.13811206]\n",
            " [ 0.14809799]]\n",
            "Mean Error:0.10582164657032124\n",
            "error before slope of sigmoid of values in l1\n",
            "[[-0.10975431]\n",
            " [-0.0914018 ]\n",
            " [ 0.07391609]\n",
            " [ 0.08910196]]\n",
            "l1 itself (dot product of l0 and weight (weighted sum))\n",
            "[[0.10975431]\n",
            " [0.0914018 ]\n",
            " [0.92608391]\n",
            " [0.91089804]]\n",
            "error after slope of sigmoid of values in l1 is multiplied times l1 error\n",
            "[[-0.01072391]\n",
            " [-0.00759069]\n",
            " [ 0.00505974]\n",
            " [ 0.00723177]]\n",
            "Mean Error:0.00765152685655693\n",
            "error before slope of sigmoid of values in l1\n",
            "[[-0.07532702]\n",
            " [-0.06151182]\n",
            " [ 0.04977973]\n",
            " [ 0.06113161]]\n",
            "l1 itself (dot product of l0 and weight (weighted sum))\n",
            "[[0.07532702]\n",
            " [0.06151182]\n",
            " [0.95022027]\n",
            " [0.93886839]]\n",
            "error after slope of sigmoid of values in l1 is multiplied times l1 error\n",
            "[[-0.00524674]\n",
            " [-0.00355096]\n",
            " [ 0.00235467]\n",
            " [ 0.00350862]]\n",
            "Mean Error:0.0036652474129137665\n",
            "error before slope of sigmoid of values in l1\n",
            "[[-0.06044451]\n",
            " [-0.04912604]\n",
            " [ 0.03979486]\n",
            " [ 0.04907442]]\n",
            "l1 itself (dot product of l0 and weight (weighted sum))\n",
            "[[0.06044451]\n",
            " [0.04912604]\n",
            " [0.96020514]\n",
            " [0.95092558]]\n",
            "error after slope of sigmoid of values in l1 is multiplied times l1 error\n",
            "[[-0.0034327 ]\n",
            " [-0.00229481]\n",
            " [ 0.00152061]\n",
            " [ 0.00229011]]\n",
            "Mean Error:0.0023845584911188983\n",
            "error before slope of sigmoid of values in l1\n",
            "[[-0.05175173]\n",
            " [-0.04198947]\n",
            " [ 0.03403931]\n",
            " [ 0.0420343 ]]\n",
            "l1 itself (dot product of l0 and weight (weighted sum))\n",
            "[[0.05175173]\n",
            " [0.04198947]\n",
            " [0.96596069]\n",
            " [0.9579657 ]]\n",
            "error after slope of sigmoid of values in l1 is multiplied times l1 error\n",
            "[[-0.00253964]\n",
            " [-0.00168908]\n",
            " [ 0.00111923]\n",
            " [ 0.00169261]]\n",
            "Mean Error:0.0017601419582090132\n",
            "error before slope of sigmoid of values in l1\n",
            "[[-0.04590957]\n",
            " [-0.03722323]\n",
            " [ 0.03019293]\n",
            " [ 0.03730225]]\n",
            "l1 itself (dot product of l0 and weight (weighted sum))\n",
            "[[0.04590957]\n",
            " [0.03722323]\n",
            " [0.96980707]\n",
            " [0.96269775]]\n",
            "error after slope of sigmoid of values in l1 is multiplied times l1 error\n",
            "[[-0.00201093]\n",
            " [-0.00133399]\n",
            " [ 0.00088409]\n",
            " [ 0.00133955]]\n",
            "Mean Error:0.0013921402341793452\n",
            "error before slope of sigmoid of values in l1\n",
            "[[-0.0416479 ]\n",
            " [-0.03375824]\n",
            " [ 0.02739494]\n",
            " [ 0.03384954]]\n",
            "l1 itself (dot product of l0 and weight (weighted sum))\n",
            "[[0.0416479 ]\n",
            " [0.03375824]\n",
            " [0.97260506]\n",
            " [0.96615046]]\n",
            "error after slope of sigmoid of values in l1 is multiplied times l1 error\n",
            "[[-0.00166231]\n",
            " [-0.00110115]\n",
            " [ 0.00072992]\n",
            " [ 0.00110701]]\n",
            "Mean Error:0.0011500960957442\n",
            "error before slope of sigmoid of values in l1\n",
            "[[-0.03836706]\n",
            " [-0.03109611]\n",
            " [ 0.02524409]\n",
            " [ 0.03119075]]\n",
            "l1 itself (dot product of l0 and weight (weighted sum))\n",
            "[[0.03836706]\n",
            " [0.03109611]\n",
            " [0.97475591]\n",
            " [0.96880925]]\n",
            "error after slope of sigmoid of values in l1 is multiplied times l1 error\n",
            "[[-0.00141555]\n",
            " [-0.0009369 ]\n",
            " [ 0.00062118]\n",
            " [ 0.00094252]]\n",
            "Mean Error:0.0009790371574623868\n",
            "error before slope of sigmoid of values in l1\n",
            "[[-0.03574306]\n",
            " [-0.02896968]\n",
            " [ 0.02352521]\n",
            " [ 0.02906372]]\n",
            "l1 itself (dot product of l0 and weight (weighted sum))\n",
            "[[0.03574306]\n",
            " [0.02896968]\n",
            " [0.97647479]\n",
            " [0.97093628]]\n",
            "error after slope of sigmoid of values in l1 is multiplied times l1 error\n",
            "[[-0.0012319 ]\n",
            " [-0.00081493]\n",
            " [ 0.00054042]\n",
            " [ 0.00082015]]\n",
            "Mean Error:0.0008518493142295976\n",
            "error before slope of sigmoid of values in l1\n",
            "[[-0.03358382]\n",
            " [-0.02722134]\n",
            " [ 0.02211135]\n",
            " [ 0.02731301]]\n",
            "l1 itself (dot product of l0 and weight (weighted sum))\n",
            "[[0.03358382]\n",
            " [0.02722134]\n",
            " [0.97788865]\n",
            " [0.97268699]]\n",
            "error after slope of sigmoid of values in l1 is multiplied times l1 error\n",
            "[[-0.00108999]\n",
            " [-0.00072083]\n",
            " [ 0.0004781 ]\n",
            " [ 0.00072562]]\n",
            "Mean Error:0.0007536378325552755\n",
            "Output After Training:\n",
            "[[0.03178421]\n",
            " [0.02576499]\n",
            " [0.97906682]\n",
            " [0.97414645]]\n",
            "Weights: [[ 7.26283009]\n",
            " [-0.21614618]\n",
            " [-3.41703015]]\n",
            "Enter int array: 1 1 0\n",
            "[1, 1, 0]\n",
            "[1.]\n",
            "Enter int array: 1 0 1\n",
            "[1, 0, 1]\n",
            "[1.]\n",
            "Enter int array: 0 1 1\n",
            "[0, 1, 1]\n",
            "[0.]\n",
            "Enter int array: 0 0 0\n",
            "[0, 0, 0]\n",
            "[0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO--FYBqxxwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d548b16e-8885-43b7-d789-b10e26037e96"
      },
      "source": [
        "#Two layer neural net with output of the first layer used as input for the second layer, capable of dealing with non-correlational data\n",
        "import numpy as np\n",
        "\n",
        "def nonlin(x,deriv=False):\n",
        "\tif(deriv==True):\n",
        "\t    return x*(1-x)\n",
        "\n",
        "\treturn 1/(1+np.exp(-x))\n",
        "    \n",
        "X = np.array([[0,0,1],\n",
        "            [0,1,1],\n",
        "            [1,0,1],\n",
        "            [1,1,1]])\n",
        "                \n",
        "y = np.array([[0],\n",
        "\t\t\t[1],\n",
        "\t\t\t[1],\n",
        "\t\t\t[0]])\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "# randomly initialize our weights with mean 0\n",
        "syn0 = 2*np.random.random((3,4)) - 1\n",
        "syn1 = 2*np.random.random((4,1)) - 1\n",
        "\n",
        "for j in range(60000):\n",
        "\n",
        "\t# Feed forward through layers 0, 1, and 2\n",
        "    l0 = X\n",
        "  # It's really just 2 of the previous implementation stacked on top of each other. The output of the first layer (l1) is the input to the second layer.\n",
        "    l1 = nonlin(np.dot(l0,syn0))\n",
        "    l2 = nonlin(np.dot(l1,syn1))\n",
        "\n",
        "    # how much did we miss the target value?\n",
        "    l2_error = y - l2\n",
        "    \n",
        "    if (j% 10000) == 0:\n",
        "        print(\"Error:\" + str(np.mean(np.abs(l2_error)))) # we'll just measure the error for accuracy here taking the mean of the absolute value of the errors in the array\n",
        "        \n",
        "    # in what direction is the target value?\n",
        "    # were we really sure? if so, don't change too much.\n",
        "    l2_delta = l2_error*nonlin(l2,deriv=True)\n",
        "\n",
        "    # how much did each l1 value contribute to the l2 error (according to the weights)? This is a critical step!\n",
        "    # use the \"confidence weighted error\" from l2 to establish an error for l1. To do this, send the error across the weights from l2 to l1. \n",
        "    # This gives what you could call a \"contribution weighted error\" because we learn how much each node value in l1 \"contributed\" to the error in l2. This step is called \"backpropagating\" and is the namesake of the algorithm. We then update syn0 using the same steps we did in the 2 layer implementation.\n",
        "    l1_error = l2_delta.dot(syn1.T)\n",
        "    \n",
        "    # in what direction is the target l1?\n",
        "    # were we really sure? if so, don't change too much.\n",
        "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
        "\n",
        "    syn1 += l1.T.dot(l2_delta)\n",
        "    syn0 += l0.T.dot(l1_delta)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error:0.4964100319027255\n",
            "Error:0.008584525653247157\n",
            "Error:0.0057894598625078085\n",
            "Error:0.004629176776769985\n",
            "Error:0.0039587652802736475\n",
            "Error:0.003510122567861678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9tWC0byeBEY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4fd9ac91-469a-48a9-83ef-1db1dc46aef1"
      },
      "source": [
        "import numpy as np\n",
        "#This introduces hyperparameters of alpha or learning rate and size of hidden layers.\n",
        "#Challenge: find the best hyperparameters for alpha and hidden layer sizes in this two layer neural net with backpropogation through gradient descent\n",
        "#Bonus extend this to an artibray user enter number of hidden layers with user entered sizes\n",
        "#Reflect: Why might it be good to start with a large alpha and decrease it over epochs?  This is one of the things that an optimizer does in tensorflow and keras!\n",
        "alphas = [0.001,0.01,0.1,1,10,100,1000]\n",
        "hiddenSizes = [4,8,16,32]\n",
        "\n",
        "# compute sigmoid nonlinearity\n",
        "def sigmoid(x):\n",
        "    output = 1/(1+np.exp(-x))\n",
        "    return output\n",
        "\n",
        "# convert output of sigmoid function to its derivative\n",
        "def sigmoid_output_to_derivative(output):\n",
        "    return output*(1-output)\n",
        "    \n",
        "X = np.array([[0,0,1],\n",
        "            [0,1,1],\n",
        "            [1,0,1],\n",
        "            [1,1,1]])\n",
        "                \n",
        "y = np.array([[0],\n",
        "\t\t\t[1],\n",
        "\t\t\t[1],\n",
        "\t\t\t[0]])\n",
        "\n",
        "for alpha in alphas:\n",
        "  for hiddenSize in hiddenSizes:\n",
        "    print(\"\\nTraining With Alpha:\" + str(alpha) + \" and hidden layer size: \" + str(hiddenSize))\n",
        "    np.random.seed(1)\n",
        "\n",
        "    # randomly initialize our weights with mean 0\n",
        "    synapse_0 = 2*np.random.random((3,hiddenSize)) - 1\n",
        "    synapse_1 = 2*np.random.random((hiddenSize,1)) - 1\n",
        "\n",
        "    for j in range(60000):\n",
        "\n",
        "        # Feed forward through layers 0, 1, and 2\n",
        "        layer_0 = X\n",
        "        layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
        "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
        "\n",
        "        # how much did we miss the target value?\n",
        "        layer_2_error = layer_2 - y\n",
        "\n",
        "        if (j % 20000) == 0:\n",
        "            print(\"Error after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))))\n",
        "\n",
        "        # in what direction is the target value?\n",
        "        # were we really sure? if so, don't change too much.\n",
        "        layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2)\n",
        "\n",
        "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
        "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
        "\n",
        "        # in what direction is the target l1?\n",
        "        # were we really sure? if so, don't change too much.\n",
        "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
        "\n",
        "        synapse_1 -= alpha * (layer_1.T.dot(layer_2_delta))\n",
        "        synapse_0 -= alpha * (layer_0.T.dot(layer_1_delta))\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training With Alpha:0.001 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.4935960431880486\n",
            "Error after 40000 iterations:0.48910016654420474\n",
            "\n",
            "Training With Alpha:0.001 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.4960339829421634\n",
            "Error after 40000 iterations:0.49230609024302613\n",
            "\n",
            "Training With Alpha:0.001 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.4915701638369947\n",
            "Error after 40000 iterations:0.4835511920051555\n",
            "\n",
            "Training With Alpha:0.001 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.48497630702745953\n",
            "Error after 40000 iterations:0.46903846539028254\n",
            "\n",
            "Training With Alpha:0.01 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.359097202563399\n",
            "Error after 40000 iterations:0.14307065901337032\n",
            "\n",
            "Training With Alpha:0.01 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.33871559263379203\n",
            "Error after 40000 iterations:0.11085529399713788\n",
            "\n",
            "Training With Alpha:0.01 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.2378006592135859\n",
            "Error after 40000 iterations:0.090147768216335\n",
            "\n",
            "Training With Alpha:0.01 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.14693984546475994\n",
            "Error after 40000 iterations:0.06514781927504919\n",
            "\n",
            "Training With Alpha:0.1 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.024098994228521613\n",
            "Error after 40000 iterations:0.014987616272210912\n",
            "\n",
            "Training With Alpha:0.1 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.024038515823849808\n",
            "Error after 40000 iterations:0.015353573121544378\n",
            "\n",
            "Training With Alpha:0.1 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.02386891164981858\n",
            "Error after 40000 iterations:0.015406466690561896\n",
            "\n",
            "Training With Alpha:0.1 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.019063872533418433\n",
            "Error after 40000 iterations:0.012389242990471293\n",
            "\n",
            "Training With Alpha:1 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.0057894598625078085\n",
            "Error after 40000 iterations:0.0039587652802736475\n",
            "\n",
            "Training With Alpha:1 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.006064058766005624\n",
            "Error after 40000 iterations:0.0041540985823973085\n",
            "\n",
            "Training With Alpha:1 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.006081688500988915\n",
            "Error after 40000 iterations:0.004147665245792007\n",
            "\n",
            "Training With Alpha:1 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.004972517050388162\n",
            "Error after 40000 iterations:0.0033864102198316558\n",
            "\n",
            "Training With Alpha:10 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.0021445955798521767\n",
            "Error after 40000 iterations:0.001478214512290799\n",
            "\n",
            "Training With Alpha:10 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.0015995429469240542\n",
            "Error after 40000 iterations:0.0011113112681715655\n",
            "\n",
            "Training With Alpha:10 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.0016467187353623783\n",
            "Error after 40000 iterations:0.001143653434163994\n",
            "\n",
            "Training With Alpha:10 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.001532635713923703\n",
            "Error after 40000 iterations:0.001054907732624855\n",
            "\n",
            "Training With Alpha:100 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.12533033352910083\n",
            "Error after 40000 iterations:0.12523107366284103\n",
            "\n",
            "Training With Alpha:100 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.5000008759677884\n",
            "Error after 40000 iterations:0.5000009297529785\n",
            "\n",
            "Training With Alpha:100 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n",
            "\n",
            "Training With Alpha:100 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n",
            "\n",
            "Training With Alpha:1000 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n",
            "\n",
            "Training With Alpha:1000 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n",
            "\n",
            "Training With Alpha:1000 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n",
            "\n",
            "Training With Alpha:1000 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}