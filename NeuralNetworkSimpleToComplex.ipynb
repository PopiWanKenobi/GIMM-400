{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworkSimpleToComplex.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielhampikian/GIMM-400/blob/master/NeuralNetworkSimpleToComplex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbKpfYxHSs3Q",
        "colab_type": "text"
      },
      "source": [
        "The following examples are artificial neural networks that learn to classify inputs of binary (the 'rules' of a particular logical expression) and associate them with a binary output (the 'answer' where if given some binary inputs we should expect a certian binary output as defined by the rules)\n",
        "\n",
        "The datasets are simple and the neural networks begin with the simplest formulation and gradually introduce complexity until there are multiple layers of neurons, a learning rate, backpropogation and gradient descent, and use of external libraries frequently encountered in ML.  This example is designed to show you on a theoretical level what goes on behind the scenes when you use a library like tensorflow and keras.  Optimizers in these libraries govern the way that weights are updated, types of layers specify the way the neurons are related to each other, weights are updated according to a learning rate and usually with backpropogation according to the type of loss function you specify (this loss is a simple taking the root of the mean difference and squaring it) and epochs and batches are the same, as is the basic structure of a neuron as a connected node of weights that get updated according to some activation function (activation functions are specified in tensorflow when you set up a layer, and the most common ones used are all coded from scratch below along with their derivates for use in backprogration).\n",
        "\n",
        "To get a feel for the way data is usually represented in neural networks in ML with python, it is vital to first understand the basics of the python NumPy library:\n",
        "\n",
        "NumPy is the main package for scientific computations in python and has been a major backbone of Python applications in various computational, engineering, scientific, statistical, image processing, etc fields. NumPy was built from 2 earlier libraries: Numeric and Numarray.\n",
        "Most deep learning algorithms make use of several numpy operations and functions. This is because compared with pure python syntax, NumPy computations are faster. NumPy for instance makes use of vectorization that enables the elimination of unnecessary loops in a code structure, hence reducing latency in execution of code. The following is an example of vectorization for a 1-d array NumPy dot operation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0hB0jFaS0B5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with vectorization\n",
        "import numpy as np #always remember to import numpy\n",
        "import time\n",
        "t0 = time.process_time()\n",
        "#array1\n",
        "tmatrix1 = [1,2,3,4,5]\n",
        "#array2\n",
        "tmatrix2 = [6,7,8,9,10]\n",
        "#dot matrix\n",
        "dt = np.dot(tmatrix1,tmatrix2)\n",
        "t1 = time.process_time()\n",
        "print (\"dot operation = \" + str(dt) + \"\\n Computation time = \" + str(1000*(t1 - t0)) + \"ms\")\n",
        "\n",
        "#without vectorization\n",
        "t0 = time.process_time()\n",
        "dot = 0\n",
        "for i in range(len(tmatrix1)):\n",
        "  %time dot+= tmatrix1[i]*tmatrix2[i]\n",
        "t1 = time.process_time()\n",
        "print (\"dot operation = \" + str(dot) + \"\\n Computation time = \" + str(1000*(t1 - t0)) + \"ms\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwosbnhFS6JL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "#create a 1-d array of 1,2,3\n",
        "mvector = np.array([1,2,3])\n",
        "print(mvector)\n",
        "#create a 2x3 matrix/2-d array\n",
        "mmatrix= np.array([[1,2,3],[4,5,6]])\n",
        "print(mmatrix)\n",
        "#print array type\n",
        "print(type(mmatrix))\n",
        "#print array shape\n",
        "print(mmatrix.shape)\n",
        "#print array size\n",
        "print(mmatrix.size)\n",
        "#print smallest number in a matrix\n",
        "print(mmatrix.min())\n",
        "#print biggest number\n",
        "print(mmatrix.max())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8_n5sU5rtdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "#Adding, subtracting and multiplying\n",
        "#array1\n",
        "mmatrix1 = np.array([[1,2,3],[4,5,6]])\n",
        "#array2\n",
        "mmatrix2 = np.array([[6,7,8],[8,9,10]])\n",
        "#add matrices\n",
        "addmatrix = np.add(mmatrix1,mmatrix2)\n",
        "print(\"addmatrix \\n\", addmatrix)\n",
        "#subtract matrices\n",
        "submatrix = np.subtract(mmatrix1,mmatrix2)\n",
        "print(\"submatrix \\n\", submatrix)\n",
        "#multiply matrices\n",
        "mulmatrix = np.multiply(mmatrix1,mmatrix2)\n",
        "print(\"mulmatrix \\n\", mulmatrix)\n",
        "#Calculating dot products\n",
        "#array1\n",
        "dmatrix1 = np.array([[1,2,3],[4,5,6]])\n",
        "#array2\n",
        "dmatrix2 = np.array([[6,7],[8,9],[9,10]])\n",
        "print(\"array1 \\n\", dmatrix1)\n",
        "print(\"array2 \\n\", dmatrix2)\n",
        "#dot matrix\n",
        "dotmatrix = np.dot(dmatrix1,dmatrix2)\n",
        "print(\"dotmatrix \\n\", dotmatrix)\n",
        "#Finding the exponential of a matrix\n",
        "mexp = np.exp(mmatrix)\n",
        "print(mexp)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBsaA_WHSIqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Simple Feedforward Artificial Neural Network: no external libraries like tensorflow, pandas, or keras:\n",
        "# No backpropogation or optimization:\n",
        "\n",
        "epoch = 500 #number of times you will run the neural net through the entire training set\n",
        "lr = 1 #learning rate \n",
        "bias = 1 #value of bias \n",
        "weights = [random.random(),random.random(),random.random()] #weights generated in a list (3 weights in total for 2 neurons and the bias)\n",
        "print(\"Weights before training: \" + str(weights))\n",
        "\n",
        "def NeuralNet(weights, inputNeuron1, inputNeuron2, expectedOutput):\n",
        "  outputActual = inputNeuron1*weights[0]+inputNeuron2*weights[1]+bias*weights[2] #usually this will be done with vectorization and matrix dot product using numpy, but so you can see what actually goes on when weights are updated I've used arrays here\n",
        "  outputActual = ApplySigmoidActivation(outputActual)\n",
        "  error = expectedOutput - outputActual #simple error measurement\n",
        "  #inference: here we train the hidden layer which is just an array of weights.\n",
        "  weights[0] += error * inputNeuron1 * lr\n",
        "  weights[1] += error * inputNeuron2 * lr\n",
        "  weights[2] += error * bias * lr\n",
        "  \n",
        "def ApplySigmoidActivation(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "  \n",
        "def ApplySigmoidDerivative(x):\n",
        "  return  x*(1-x)\n",
        "\n",
        "def ApplyReluActivation(x):\n",
        "   return np.maximum(0,x)\n",
        "  \n",
        "def ApplyReluDerivative(x):\n",
        "  if x<=0:\n",
        "    x = 0\n",
        "  else:\n",
        "    x = 1\n",
        "  return x\n",
        "\n",
        "def softmax(x):\n",
        "  expo = np.exp(x)\n",
        "  expo_sum = np.sum(np.exp(x))\n",
        "  return expo/expo_sum\n",
        "\n",
        "#print (softmax(mmatrix))\n",
        "\n",
        "\n",
        "for i in range(epoch):\n",
        "  NeuralNet(weights, 1,1,1) #True or true input should return true\n",
        "  NeuralNet(weights, 1,0,1) #True or false input should return true\n",
        "  NeuralNet(weights, 0,1,1) #False or true input should return true\n",
        "  NeuralNet(weights, 0,0,0) #False or false input should return false\n",
        "  if(i % 200== 0):\n",
        "    print(\"weights currently at: \" + str(weights))\n",
        "  \n",
        "print(\"Weights after training: \" + str(weights))\n",
        "while True: \n",
        "  x = int(input())\n",
        "  y = int(input())\n",
        "  output = x*weights[0] + y*weights[1] + bias*weights[2]\n",
        "  print(output)\n",
        "  output = ApplyReluDerivative(output) #relu derivative activation function for output because we have binary classification\n",
        "\n",
        "  print(x, \"or\", y, \"is : \", output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEEZRWMWkeGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Slightly more complex single layer Neural Network: still no external libraries like tensorflow, pandas, or keras:\n",
        "import numpy as np #import numpy the linear algebra library\n",
        "\n",
        "# sigmoid function - introduces non-linearity into the neural net (not 1-1 input output)\n",
        "#sigmoid maps to a value between 0 and 1 to convert numbers to probabilities\n",
        "def nonlin(x,deriv=False):\n",
        "    if(deriv==True): # when true, this maps the derivative of the sigmoid function - so we can get the slope at a given point\n",
        "        return x*(1-x)\n",
        "    return 1/(1+np.exp(-x))\n",
        "    \n",
        "# input dataset\n",
        "X = np.array([  [0,0,1],\n",
        "                [0,1,1],\n",
        "                [1,0,1],\n",
        "                [1,1,1]])\n",
        "    \n",
        "# output dataset            \n",
        "y = np.array([[0,0,1,1]]).T\n",
        "\n",
        "# seed random numbers to make calculation\n",
        "# deterministic (just a good practice)\n",
        "np.random.seed(1)\n",
        "epochs = 50000\n",
        "weights = np.array([[0,0,0,0]]).T\n",
        "# initialize weights randomly with mean 0\n",
        "#we only have an input and output layer, so we only need on matrix of wieghts to connect them with dimensions (3,1) 3 inputs to 1 output\n",
        "#if we wanted to change the number of inputs we would also change this (3,1) to reflect inputs, but we generally want 1 output since we're doing a regresion neural net\n",
        "#syn0 = 2*np.random.random((3,1)) - 1\n",
        "\n",
        "def neuralNet(l0, epochs, predict, userTest=[0,0,0], testWeights=[0,0,0,0]):\n",
        "  if(not predict):\n",
        "    for iter in range(epochs):\n",
        "\n",
        "      # forward propagation - the prediction step, we let the network try to predict it's output and measure the error\n",
        "      syn0 = testWeights\n",
        "      #this is the 'weighted sum step where we use the dot or cross product of two matrices to output a matrix that has the multiplied and summed total of the weights and values for that row and column combination, and then we convert that back into a number since we get a probability with the sigmoid function'\n",
        "      #the dimensions going on in the next step are: (4X3)dot(3X1) = (4X1) so everything in the matrix gets multplied and hte result is a matix with the number of rows in first matrix and number of columns in second\n",
        "      l1 = nonlin(np.dot(l0,syn0)) #multiplies 10 input by syn0 weights, then passes it through our sigmoid function to convert the numbers to probabilities \n",
        "      \n",
        "\n",
        "      # compare our guess - l1 - to the actual answer - y -\n",
        "      l1_error = y - l1\n",
        "      if(iter % 100 == 0):\n",
        "        print('error before slope of sigmoid of values in l1')\n",
        "        print(l1_error)\n",
        "        print(\"l1 itself (dot product of l0 and weight (weighted sum))\")\n",
        "        print(l1)\n",
        "    \n",
        "\n",
        "      # multiply how much we missed by the \n",
        "      # slope of the sigmoid at the values in l1 - basically we multiply elementwise a 4,1 matrix with another 4,1 matrix of its sigmoid derivatives thereby reducing the error of high confidence predictions.\n",
        "      # if the network has a very confident guess (slope is very shallow or close to 0), we leave it alone with the multiplication here but if it's closer to .5 then we heavily update the guess with this multiplication\n",
        "      l1_delta = l1_error * nonlin(l1,True)\n",
        "      if(iter%100 == 0):\n",
        "        print('error after slope of sigmoid of values in l1 is multiplied times l1 error')\n",
        "        print(l1_delta)\n",
        "        print(\"Mean Error:\" + str(np.mean(np.abs(l1_delta)))) # we'll just measure the error for accuracy here taking the mean of the absolute value of the errors in the array\n",
        "\n",
        "    \n",
        "\n",
        "     # update weights (again cross multiplication , added to all weights between the two matrix)\n",
        "      syn0 += np.dot(l0.T,l1_delta)\n",
        "    return syn0,l1\n",
        "    \n",
        "  if predict:           \n",
        "          \n",
        "    syn0 = testWeights\n",
        "    total = 0    #the dimensions goig on in the next step are: (4X3)dot(3X1) = (4X1) so everything in the matrix gets multplied and hte result is a matix with the number of rows in first matrix and number of columns in second\n",
        "    for i in range(len(userTest)):\n",
        "      #multiplies 10 input by syn0 weights, then passes it through our sigmoid function to convert the numbers to probabilities \n",
        "    #print(\"hey\" + str(l1))\n",
        "      total += userTest[i] * testWeights[i]\n",
        "        # compare our guess - l1 - to the actual answer - y -\n",
        "        # l1_error = y - l1\n",
        "        #l1_delta = l1_error * nonlin(l1,True)\n",
        "        \n",
        "    \n",
        "\n",
        "      # update weights (again cross multiplication , added to all weights between the two matrix)\n",
        "        #syn0 += np.dot(l0.T,l1_delta)\n",
        "     # return the prediction:\n",
        "    return np.around(nonlin(total),decimals=0)\n",
        "      \n",
        "w,l1 = neuralNet(X,1000, False, testWeights = 2*np.random.random((3,1)) - 1)\n",
        "print(\"Output After Training:\")\n",
        "print(str(l1))\n",
        "print(\"Weights: \" + str(w))\n",
        "#user test:\n",
        "while True:\n",
        "  userInput = input(\"Enter int array: \")\n",
        "  userArray = list(map(int, userInput.split()))\n",
        "  print(userArray)\n",
        "  print(str(neuralNet(X,1,True, userTest=userArray, testWeights=w)))\n",
        "#Note that adding a 0 0 0 learning set and a 0 1 0 doesn't work yet, we need to introduce two hyperparameters, layers, numbers of neurons, and an alpha or (learning rate)\n",
        "#Try 1 1 0 which is not in the data set, but if assume the rule it's trying to infer is perfect correspondence to the first column should return 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO--FYBqxxwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Two layer neural net with output of the first layer used as input for the second layer, capable of dealing with non-correlational data\n",
        "import numpy as np\n",
        "\n",
        "def nonlin(x,deriv=False):\n",
        "\tif(deriv==True):\n",
        "\t    return x*(1-x)\n",
        "\n",
        "\treturn 1/(1+np.exp(-x))\n",
        "    \n",
        "X = np.array([[0,0,1],\n",
        "            [0,1,1],\n",
        "            [1,0,1],\n",
        "            [1,1,1]])\n",
        "                \n",
        "y = np.array([[0],\n",
        "\t\t\t[1],\n",
        "\t\t\t[1],\n",
        "\t\t\t[0]])\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "# randomly initialize our weights with mean 0\n",
        "syn0 = 2*np.random.random((3,4)) - 1\n",
        "syn1 = 2*np.random.random((4,1)) - 1\n",
        "\n",
        "for j in range(60000):\n",
        "\n",
        "\t# Feed forward through layers 0, 1, and 2\n",
        "    l0 = X\n",
        "  # It's really just 2 of the previous implementation stacked on top of each other. The output of the first layer (l1) is the input to the second layer.\n",
        "    l1 = nonlin(np.dot(l0,syn0))\n",
        "    l2 = nonlin(np.dot(l1,syn1))\n",
        "\n",
        "    # how much did we miss the target value?\n",
        "    l2_error = y - l2\n",
        "    \n",
        "    if (j% 10000) == 0:\n",
        "        print(\"Error:\" + str(np.mean(np.abs(l2_error)))) # we'll just measure the error for accuracy here taking the mean of the absolute value of the errors in the array\n",
        "        \n",
        "    # in what direction is the target value?\n",
        "    # were we really sure? if so, don't change too much.\n",
        "    l2_delta = l2_error*nonlin(l2,deriv=True)\n",
        "\n",
        "    # how much did each l1 value contribute to the l2 error (according to the weights)? This is a critical step!\n",
        "    # use the \"confidence weighted error\" from l2 to establish an error for l1. To do this, send the error across the weights from l2 to l1. \n",
        "    # This gives what you could call a \"contribution weighted error\" because we learn how much each node value in l1 \"contributed\" to the error in l2. This step is called \"backpropagating\" and is the namesake of the algorithm. We then update syn0 using the same steps we did in the 2 layer implementation.\n",
        "    l1_error = l2_delta.dot(syn1.T)\n",
        "    \n",
        "    # in what direction is the target l1?\n",
        "    # were we really sure? if so, don't change too much.\n",
        "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
        "\n",
        "    syn1 += l1.T.dot(l2_delta)\n",
        "    syn0 += l0.T.dot(l1_delta)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9tWC0byeBEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "#This introduces hyperparameters of alpha or learning rate and size of hidden layers.\n",
        "#Challenge: find the best hyperparameters for alpha and hidden layer sizes in this two layer neural net with backpropogation through gradient descent\n",
        "#Bonus extend this to an artibray user enter number of hidden layers with user entered sizes\n",
        "#Reflect: Why might it be good to start with a large alpha and decrease it over epochs?  This is one of the things that an optimizer does in tensorflow and keras!\n",
        "alphas = [0.001,0.01,0.1,1,10,100,1000]\n",
        "hiddenSizes = [4,8,16,32]\n",
        "\n",
        "# compute sigmoid nonlinearity\n",
        "def sigmoid(x):\n",
        "    output = 1/(1+np.exp(-x))\n",
        "    return output\n",
        "\n",
        "# convert output of sigmoid function to its derivative\n",
        "def sigmoid_output_to_derivative(output):\n",
        "    return output*(1-output)\n",
        "    \n",
        "X = np.array([[0,0,1],\n",
        "            [0,1,1],\n",
        "            [1,0,1],\n",
        "            [1,1,1]])\n",
        "                \n",
        "y = np.array([[0],\n",
        "\t\t\t[1],\n",
        "\t\t\t[1],\n",
        "\t\t\t[0]])\n",
        "\n",
        "for alpha in alphas:\n",
        "  for hiddenSize in hiddenSizes:\n",
        "    print(\"\\nTraining With Alpha:\" + str(alpha) + \" and hidden layer size: \" + str(hiddenSize))\n",
        "    np.random.seed(1)\n",
        "\n",
        "    # randomly initialize our weights with mean 0\n",
        "    synapse_0 = 2*np.random.random((3,hiddenSize)) - 1\n",
        "    synapse_1 = 2*np.random.random((hiddenSize,1)) - 1\n",
        "\n",
        "    for j in range(60000):\n",
        "\n",
        "        # Feed forward through layers 0, 1, and 2\n",
        "        layer_0 = X\n",
        "        layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
        "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
        "\n",
        "        # how much did we miss the target value?\n",
        "        layer_2_error = layer_2 - y\n",
        "\n",
        "        if (j % 20000) == 0:\n",
        "            print(\"Error after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))))\n",
        "\n",
        "        # in what direction is the target value?\n",
        "        # were we really sure? if so, don't change too much.\n",
        "        layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2)\n",
        "\n",
        "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
        "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
        "\n",
        "        # in what direction is the target l1?\n",
        "        # were we really sure? if so, don't change too much.\n",
        "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
        "\n",
        "        synapse_1 -= alpha * (layer_1.T.dot(layer_2_delta))\n",
        "        synapse_0 -= alpha * (layer_0.T.dot(layer_1_delta))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}